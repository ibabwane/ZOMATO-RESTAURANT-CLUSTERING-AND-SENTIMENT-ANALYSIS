# Tokenizer
from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words="MAX_NB_WORDS")
tokenizer.fit_on_texts(df_name[['Cuisines','Links','Timings']])

import re, string, unicodedata
import nltk
import inflect
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer

df_name.head()

df_name['Cuisines']= df_name['Cuisines'].astype(str)

# Functions
def remove_non_ascii(words):
    """Remove non-ASCII characters from list of tokenized words"""
    new_words = []
    for word in words:
      word = str(word)
      new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')
      new_words.append(new_word)
    return new_words

def to_lowercase(words):
    """Convert all characters to lowercase from list of tokenized words"""
    new_words = []
    for word in words:
        new_word = word.lower()
        new_words.append(new_word)
    return new_words

def remove_punctuation(words):
    """Remove punctuation from list of tokenized words"""
    new_words = []
    for word in words:
        new_word = re.sub(r'[^\w\s]', '', word)
        if new_word != '':
            new_words.append(new_word)
    return new_words

def replace_numbers(words):
    """Replace all interger occurrences in list of tokenized words with textual representation"""
    p = inflect.engine()
    new_words = []
    for word in words:
        if word.isdigit():
            new_word = p.number_to_words(word)
            new_words.append(new_word)
        else:
            new_words.append(word)
    return new_words

def stem_words(words):
    """Stem words in list of tokenized words"""
    stemmer = LancasterStemmer()
    stems = []
    for word in words:
        stem = stemmer.stem(word)
        stems.append(stem)
    return stems

def lemmatize_verbs(words):
    """Lemmatize verbs in list of tokenized words"""
    lemmatizer = WordNetLemmatizer()
    lemmas = []
    for word in words:
        lemma = lemmatizer.lemmatize(word, pos='v')
        lemmas.append(lemma)
    return lemmas

def normalize(words):
    words = remove_non_ascii(words)
    words = to_lowercase(words)
    words = remove_punctuation(words)
    words = replace_numbers(words)
    return words

# Applying functions
col_lsts = ['Links','Cuisines','Timings']
for col_lst in col_lsts:
  df_name[col_lst] = normalize(df_name[col_lst])

df_name.head()

# Preprocessing the dataset for Clustering models

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score

# Tfidf vectorizer
vectorizer = TfidfVectorizer(stop_words= 'english')
X = vectorizer.fit_transform(df_name['Cuisines'])

X.shape

# Type of X object
type(X)

# Converting 'X' object to array
X = X.toarray()

type(X)

from yellowbrick.cluster import KElbowVisualizer

# Function to find appropriate 'K' value
def KElbowvisualizer(metric):
  model = KMeans(max_iter=300,random_state=0)
  plt.figure(figsize=(10,5))
  sns.set(font_scale = 1)
  visualizer = KElbowVisualizer(model, k=(2,20),metric= metric, timings= False, locate_elbow= True)
  # # Fit the data to the visualize
  visualizer.fit(X)  
  visualizer.poof()

# KElbowvisualizer with metric as 'calinski_harabasz'
KElbowvisualizer('calinski_harabasz')

# KElbowvisualizer with metric as 'silhouette'
KElbowvisualizer('silhouette')

# KElbowvisualizer with metric as 'distortion
KElbowvisualizer('distortion')

## Calculate Silhoutte scores to find appropriate 'K' value

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score

import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
sillhouette_cluster_values={}

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    sillhouette_cluster_values[n_clusters] = silhouette_avg

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors, edgecolor='k')

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k')

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")
    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

plt.show()

silhouette_avg

sillhouette_cluster_values

x = list(sillhouette_cluster_values.keys())
y = list(sillhouette_cluster_values.values())
plt.figure(figsize=(10,5))
plt.plot(x, y, color='green', marker='o', linestyle='dashed', linewidth=2, markersize=12)
plt.xlabel("K - values")
plt.ylabel("Sillhouette scores")

##From the above graph we can take k = 15 as optimum value of silhouette score, as compared to Elbow method also we found k=15 as best value.

# looks like we can go with 14 clusters.
# Creating an object for K Means clustering
kmeans= KMeans(n_clusters=15, init= 'k-means++',max_iter=300, n_init=1,random_state = 0)

# MOdel fit
kmeans.fit(X)

#predict the labels of clusters.
labels = kmeans.fit_predict(X)

# Clusters center
clusters_center = kmeans.cluster_centers_

[df_name['Cuisines'][15]]

# Testin

X_new= vectorizer.transform([df_name['Cuisines'][15]])

y_pred_new= kmeans.predict(X_new)[0]

y_pred_new

# Model Validation
# silhouette score of my clusters
print("Silhouette Coefficient: %0.3f"%silhouette_score(X, kmeans.labels_))

# Creating new feature to store labels
df_name['Kmeans_labels'] = kmeans.labels_

# Creating a DataFrame for KMeans Labels visualization
Kmeans_labels_count = pd.DataFrame(df_name.groupby(['Kmeans_labels'])['Cuisines'].count()).reset_index()

Kmeans_labels_count.head()

df_name.head()

def labelsnobs(dataframe,x_value,y_value,palette_type,title,y_label):
  plt.figure(figsize = (20, 8))
  sns.set(font_scale = 2)
  sns.barplot(data = dataframe,x = x_value,y = y_value,palette=palette_type)
  plt.title(title,fontweight='bold')
  plt.ylabel(y_label)
  plt.xticks(rotation = 90,fontsize = 14)
  plt.yticks(fontsize = 18)
  plt.show()

# Visualization of Labels with n number of observations
labelsnobs(Kmeans_labels_count,Kmeans_labels_count['Kmeans_labels'],Kmeans_labels_count['Cuisines'],['#581845','#900C3F','C70039','#FF5733'],
           'labels with n number of observation', 'Observations')

# Getting terms
terms = vectorizer.get_feature_names()

# Top terms(words) in  per cluster set
print('\033[1m' + 'Top terms in each cluster:' + '\033[0m')
cuisines_list = []
order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
for i in range(15):
  num = str(i)
  # print('\033[1m' + 'Cluster:' + '\033[0m',i, end='')
  print('\033[1m' + 'Cluster:' + num + '\033[0m', end='')
  for ind in order_centroids[i, :10]:
    cuisines_list.append(terms[ind])
  print(cuisines_list)
  cuisines_list = []

# Grouping 'Kmeans_labels' and 'Name' feature
Kmeans_labels_name = pd.DataFrame(df_name.groupby(['Kmeans_labels'])['Name'],columns = ['KMeans_Cluster_labels','Restaurants'])
Kmeans_labels_name.set_index('KMeans_Cluster_labels', inplace=True)

# Reataurant names with respect to each clusters
for i in range(0,15):
  num = str(i)
  print('Cluster:'+ num)
  print(Kmeans_labels_name['Restaurants'][i])

df_name['Cuisines'].unique

Agglomerative Clustering

import inflect
import scipy.cluster.hierarchy as shc
from sklearn.cluster import AgglomerativeClustering

# Hierarchical Clustering for 'Cuisine' feature
plt.figure(figsize=(30,10))
sns.set(font_scale = 1.8)
dend = shc.dendrogram(shc.linkage(X, method='ward'))
plt.title('Dendrograms',fontweight = 'bold')
plt.xlabel('Cuisines')
plt.ylabel('Euclidean Distances')
plt.axhline(y=1.9, color='r', linestyle='--')
plt.show()


# To choose appropriate K value

for k in range(2,20):
  aggh = AgglomerativeClustering(n_clusters=k, affinity='euclidean', linkage='ward')  
  aggh.fit(X)
  y_hc=aggh.fit_predict(X)
  print("For n_clusters =", k, "The average silhouette_score is :", round(silhouette_score(X, y_hc),5))

# Creating an object for AgglomerativeClustering
aggh = AgglomerativeClustering(n_clusters=15, affinity='euclidean', linkage='ward')

# Fitting the model
aggh.fit(X)

# Label Prediction
y_hc=aggh.fit_predict(X)

# Labels
print(y_hc)

# Creating new feature to store labels
df_name['Agglomerative_labels'] = aggh.labels_

# Creating a DataFrame for KMeans Labels visualization
Agglomerative_labels_count = pd.DataFrame(df_name.groupby(['Agglomerative_labels'])['Cuisines'].count()).reset_index()

# Visualization of Labels with n number of observations
labelsnobs(Agglomerative_labels_count,Agglomerative_labels_count['Agglomerative_labels'],Agglomerative_labels_count['Cuisines'],['#581845','#900C3F','C70039','#FF5733'],
           'labels with n number of observation', 'Observations')

# Grouping 'Kmeans_labels' and 'Name' feature
Agglomerative_labels_name = pd.DataFrame(df_name.groupby(['Agglomerative_labels'])['Name'],columns = ['Agglomerative_labels','Restaurants'])
Agglomerative_labels_name.set_index('Agglomerative_labels', inplace=True)

# Reataurant names with respect to each clusters
for i in range(0,15):
  num = str(i)
  print('Cluster:'+ num)
  print(Agglomerative_labels_name['Restaurants'][i])

Loading Zomato Restaurant Reviews CSV file

data2 = pd.read_csv('/content/drive/MyDrive/Data/Zomato Restaurant reviews.csv')



df_review=data2.copy()

#Shape
df_review.shape

# Head
df_review.head()

# Info
df_review.info()

# Null values treatment

# Dropping rows with Null values
df_review = df_review.dropna(axis =0)

df_review['Rating'].unique()

#Drop row for Like as Rating
a = df_review[df_review['Rating']=='Like'].index
df_review.drop(a,inplace=True)

df_review['Rating'].unique()

##Go through all features one by one

df_review.head()

df_review.info()

# Converting 'Rating' feature to Float datatype
df_review['Rating'] = df_review['Rating'].apply(lambda x: float(x))

# Summary
df_review.describe()

# Count plot for Rating
plt.figure(figsize = (9,5))
sns.set(font_scale = 1.2)
sns.countplot(x='Rating',data = df_review, palette = ['#C21010','#E64848','#CFE8A9','#ABC9FF','#F6C6EA'])
plt.title('Rating count',fontweight = 'bold')
plt.show()

Most of the customer has given 5 rating

# Downloading stop words
nltk.download('stopwords')

# Stop words
stop_words = stopwords.words('english')
# Rest words
rest_word = ['order','restaurant','taste','ordered','good','food','table','place','one','also']

df_review['Reviews_txt_processed'] = df_review['Review']

# Wordcloud for Reviews
df_review['Reviews_txt_processed'] = df_review['Reviews_txt_processed'].map(lambda x: re.sub('[,\.!?]','', x))
df_review['Reviews_txt_processed'] = df_review['Reviews_txt_processed'].map(lambda x: x.lower())
df_review['Reviews_txt_processed'] = df_review['Reviews_txt_processed'].map(lambda x: x.split())
df_review['Reviews_txt_processed'] = df_review['Reviews_txt_processed'].apply(lambda x: [words for words in x if words not in stop_words])
df_review['Reviews_txt_processed'] = df_review['Reviews_txt_processed'].apply(lambda x: [words for words in x if words not in rest_word])
df_review['Reviews_txt_processed'] = df_review['Reviews_txt_processed'].astype(str)

df_review.head()

#Wordcloud for Reviews
ps = PorterStemmer() 
long_string = ','.join(list(df_review['Reviews_txt_processed'].values))
# print(long_string)
wordcloud = WordCloud(background_color="white", max_words=100, contour_width=13,colormap = 'magma')
wordcloud.generate(long_string)
wordcloud.to_image()

# Positive Reviews(Ratings greater than 3)
pos_rev = df_review[df_review['Rating'] > 3]

# Wordcloud for Positive reviews
long_string = ','.join(list(pos_rev['Review'].values))
long_string
wordcloud = WordCloud(background_color="white", max_words=100, contour_width=13,colormap = 'magma')
wordcloud.generate(long_string)
wordcloud.to_image()

# Extracting the number of reviews
df_review['Metadata'] = df_review['Metadata'].str.split(' ')
df_review['Metadata'] = df_review['Metadata'].apply(lambda x: x[0])
df_review['Metadata'] = df_review['Metadata'].apply(lambda x : int(x))

# Creating a dataframe to store top 10 reviewers
top_reviewers = pd.DataFrame(df_review.groupby(['Reviewer','Metadata']).sum()).reset_index()
top_reviewers = top_reviewers.sort_values(by = 'Metadata',ascending = False)
top_reviewers = top_reviewers[:15]
top_reviewers.head()

# Visualization of top 10 reviewers
plt.figure(figsize = (10,6))
sns.barplot(data = top_reviewers,x = 'Reviewer',y = 'Metadata')
plt.title('Reviews Count',fontweight = 'bold')
plt.xticks(rotation = 90)
plt.show()

# Top 10 Restaurants with high average rating
df_review['Rating'] = df_review['Rating'].apply(lambda x: float(x))
avg_ratings = pd.DataFrame(df_review.groupby('Restaurant')['Rating'].mean()).reset_index()
avg_ratings = avg_ratings.sort_values(by = 'Rating',ascending = False)
avg_ratings = avg_ratings[:10]

# Visualization of top 10 reviewers
plt.figure(figsize = (10,6))
sns.barplot(data = avg_ratings,x = 'Restaurant',y = 'Rating')
plt.title('Top Rated Restaurants',fontweight = 'bold')
plt.xticks(rotation = 90)
plt.show()

# Sentiment Analysis




# shape of two dataframes
print(df_name.shape)
print(df_review.shape)

#number of unique data frame in 'Zomato Restaurant names and Metadata' dataframe
df_name['Name'].nunique()

#number of unique data frame in 'Zomato Restaurant names and Metadata' dataframe
df_review['Restaurant'].nunique()

Zomato Restaurant names and Metadata has 105 unique Restaurants. Zomato Restaurant reviews has 100 unique Restaurants. For merging two dataframes we are to remove non-common Restaurants.

lst = []
for name in df_name['Name'].unique():
  if name not in df_review['Restaurant'].unique():
    lst.append(name)

lst

# Filtering out rows 
final = df_name
for not_com_rest_names in lst:
  final = final[final['Name'] != not_com_rest_names]

# Renaming the column 'Restaurant'
df_review.rename(columns = {'Restaurant':'Name'}, inplace = True)

# Merging two dataframes
merged_df = pd.merge(df_review, final, how='left', on='Name')

merged_df.columns

# Droping less significant features
merged_df.drop(['Time','Metadata','Links','Timings','Kmeans_labels','Agglomerative_labels'],axis = 1,inplace = True)

# Re-indexing the columns
merged_df = merged_df.reindex(columns=['index','Name', 'Reviewer','Review','Reviews_txt_processed','Review_length','Rating','Polarity','Cost','Cuisines'])

# Shape
merged_df.shape

# Head
merged_df.head(2)

# Creating dataframe to store avg rating and avg cost for each reataurant
ratingncost = pd.DataFrame(merged_df.groupby('Name')['Rating','Cost'].mean()).reset_index()
print(ratingncost)

#Visualization of Restaurant Cost vs Rating
plt.figure(figsize=(40,10))
fig = px.bar(ratingncost, x="Name", y="Cost",color="Rating",width=900, height=700)
fig.update_xaxes(ticks="outside", tickwidth=1, tickcolor='maroon',tickangle=90, ticklen=10)
fig.update_layout(title_text="Restaurant Cost vs Rating")
fig.show()

# Tokenizer
tokenizer_vader = Tokenizer(num_words="MAX_NB_WORDS")
tokenizer_vader.fit_on_texts(merged_df['Review'])

# Appling text processing function
merged_df['Review'] = merged_df['Review'].apply(normalize)

# Replacing comma
merged_df['Review'] = merged_df['Review'].apply(lambda x: ','.join(x))
merged_df['Review'] = merged_df['Review'].apply(lambda x: x.replace(',',''))

merged_df['Review']